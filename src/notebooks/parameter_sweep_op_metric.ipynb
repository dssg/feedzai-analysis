{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from matplotlib.lines import Line2D\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import itertools\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facct22.utils import get_db_con\n",
    "\n",
    "from facct22.analysis_functions import (\n",
    "    get_all_decisions, groups, schemas, users, dps, dt, pdr, group_order, colors, \n",
    "    ttests_operational_metrics, assign_conf_mat_cell, _modify_value_and_time, ttests_operational_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_file = '../../conf/credentials.yaml'\n",
    "engine = get_db_con(cred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparisons = [\n",
    "#     ('Control-A', 'Control-B'),\n",
    "#     ('Control-B', 'LIME'),\n",
    "#     ('Control-B', 'TreeInt'),\n",
    "#     ('Control-B', 'TreeSHAP'),\n",
    "#     ('Control-B', 'Random'),\n",
    "#     ('Control-B', 'Irrelevant'),\n",
    "#     ('Control-B', 'LIME'),\n",
    "#     ('Control-B', 'TreeInt'),\n",
    "#     ('Control-B', 'TreeSHAP'),\n",
    "#     ('TreeInt', 'Random'),\n",
    "#     ('TreeSHAP', 'Random'),\n",
    "#     ('LIME', 'Random'),\n",
    "#     ('TreeInt', 'Irrelevant'),\n",
    "#     ('TreeSHAP', 'Irrelevant'),\n",
    "#     ('LIME', 'Irrelevant'),\n",
    "# ]\n",
    "\n",
    "comparisons = [\n",
    "    ('Control-A', 'Control-B'),\n",
    "    ('Control-B', 'Explainer'),\n",
    "    ('Control-B', 'Random'),\n",
    "    ('Control-B', 'Irrelevant'),\n",
    "    ('Random', 'Explainer'),\n",
    "    ('Irrelevant', 'Explainer'),\n",
    "]\n",
    "\n",
    "param_grid = {\n",
    "    'fn': [-0.8, -1, -1.2, -3, -5],\n",
    "    'p_loss_trx': [0.2, 0.3, 0.4], \n",
    "    'cust_worth': [0, 1, 3, 5], \n",
    "    'p_loss_cust': [0, 0.05, 0.1, 0.3], \n",
    "    'p_return_cust': [0, 0.2, 0.3],\n",
    "    'suspicious_handling': [0, 600, 1800, 3600, 'approve', 'decline'] # If this is an int, it's correct and timepenalty\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_parameter_sweep(\n",
    "    decisions, \n",
    "    comparisons,\n",
    "    fn,\n",
    "    p_loss_trx, \n",
    "    cust_worth,\n",
    "    p_loss_cust,\n",
    "    p_return_cust,\n",
    "    suspicious_handling\n",
    "):\n",
    "    all_param_combinations = itertools.product(\n",
    "        fn, \n",
    "        p_loss_trx, \n",
    "        cust_worth, \n",
    "        p_loss_cust, \n",
    "        p_return_cust,\n",
    "        suspicious_handling\n",
    "    )\n",
    "\n",
    "    results = list()\n",
    "    significance_results = list()\n",
    "    for config in all_param_combinations:\n",
    "        \n",
    "        # suspicious handling\n",
    "        if isinstance(config[5], int):\n",
    "            add_time= config[5]\n",
    "            suspicious_strategy = 'correct'\n",
    "        else:\n",
    "            add_time= 0 # The value doesn't matter\n",
    "            suspicious_strategy = config[5]\n",
    "            \n",
    "        \n",
    "        param = {\n",
    "            'fn': config[0],\n",
    "            'p_loss_trx': config[1],\n",
    "            'cust_worth': config[2],\n",
    "            'p_loss_cust': config[3],\n",
    "            'p_return_cust': config[4],\n",
    "            'suspicious_add_time': add_time,\n",
    "            'suspicious_strategy': suspicious_strategy\n",
    "        }\n",
    "\n",
    "        metrics = list()\n",
    "        ttests = list()\n",
    "        \n",
    "        df = dt(decisions, param, suspicious_strategy, ['group'])\n",
    "        df['metric'] = 'dt'\n",
    "        \n",
    "        ttest = ttests_operational_metrics(df, comparisons)\n",
    "        ttest['metric'] = 'dt'\n",
    "        \n",
    "        metrics.append(df)\n",
    "        ttests.append(ttest)\n",
    "        \n",
    "        \n",
    "        df = dps(decisions, param, suspicious_strategy, ['group'])\n",
    "        df['metric'] = 'dps'\n",
    "        \n",
    "        ttest = ttests_operational_metrics(df, comparisons)\n",
    "        ttest['metric'] = 'dps'\n",
    "        \n",
    "        metrics.append(df)\n",
    "        ttests.append(ttest)\n",
    "        \n",
    "        df = pdr(decisions, param, suspicious_strategy, ['group'], n_samples=500, n_iterations=100)\n",
    "        df['metric'] = 'pdr'\n",
    "        \n",
    "        ttest = ttests_operational_metrics(df, comparisons)\n",
    "        ttest['metric'] = 'pdr'\n",
    "        \n",
    "        metrics.append(df)\n",
    "        ttests.append(ttest)\n",
    "        \n",
    "        res = pd.concat(metrics)\n",
    "        sig_res = pd.concat(ttests)\n",
    "        \n",
    "        # appending the parameter values\n",
    "        for p, v in param.items():\n",
    "            res[p] = v\n",
    "            sig_res[p] = v\n",
    "\n",
    "\n",
    "        results.append(res)\n",
    "        significance_results.append(sig_res)\n",
    "\n",
    "    \n",
    "    return pd.concat(results), pd.concat(significance_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if a simple model threshold was used, without a review band?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_score(score: float, rev_threshold: int = 39) -> int:\n",
    "    \"\"\"Computes the score according to the specified thresholds.\n",
    "\n",
    "    The default values correspond to the thresholds established\n",
    "    for the model.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    score : float\n",
    "        The non-normalized as retrieved by the model, i.e., in the\n",
    "        range [0, 1], or in the range [0, 1000].\n",
    "\n",
    "    rev_threshold : int\n",
    "        The non-normalized review threshold below which any transaction\n",
    "        will be automatically accepted (identified as legitimate).\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The normalized score between [0, 1000].\n",
    "    \"\"\"\n",
    "    if 0 < score <= 1:\n",
    "        score *= 1000\n",
    "\n",
    "    # Compute the score for the given thresholds\n",
    "    if score < rev_threshold:\n",
    "        score = 500 * score / rev_threshold\n",
    "    else:\n",
    "        threshold_nnorm = 500 / (1000 - rev_threshold)\n",
    "        score = threshold_nnorm * score + 1000 * (1 - threshold_nnorm)\n",
    "\n",
    "    # Since CM multiplies by 1000, we have to divide by 1000\n",
    "    return round(score)\n",
    "\n",
    "\n",
    "def fetch_trx_scores(db_conn, groups, schemas):\n",
    "    qs = [\n",
    "        f\"\"\"\n",
    "            with unique_trx as (\n",
    "                select\n",
    "                    trx_id, trx_score, trx_label \n",
    "                from openxai{schema[9:]}.explanation_components\n",
    "                group by 1, 2, 3\n",
    "            )\n",
    "            select distinct on (trx_id)\n",
    "                trx_id, xplz_id, (label_fields -> 'amount_usd')::int as trx_amnt, \n",
    "                trx_score, label, rf.\"group\"\n",
    "            from {schema}.trx_user_assignments join unique_trx using(trx_id)\n",
    "            join {schema}.review_feedback rf using(xplz_id)\n",
    "            --where rf.\"group\" in {tuple(groups)}\n",
    "            order by trx_id\n",
    "        \"\"\"\n",
    "        for schema in schemas\n",
    "    ]\n",
    "    \n",
    "    return pd.concat([pd.read_sql(q, db_conn) for q in qs])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def assign_conf_mat(score, label, threshold):\n",
    "        if score > threshold:\n",
    "            return 'tp' if label==1 else 'fp'\n",
    "        else:\n",
    "            return 'fn' if label==1 else 'tn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trx_id</th>\n",
       "      <th>xplz_id</th>\n",
       "      <th>trx_amnt</th>\n",
       "      <th>trx_score</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [trx_id, xplz_id, trx_amnt, trx_score, label, group]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_trx_scores(engine, group_order, schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ducktales",
   "language": "python",
   "name": "ducktales"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
